{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import numpy as np\n",
    "from config import *\n",
    "import pandas as pd\n",
    "\n",
    "class TweetDataset():\n",
    "    def __init__(self, text, selected_text, sentiment, tokenizer, max_len):\n",
    "        self.text = text\n",
    "        self.selected_text = selected_text\n",
    "        self.sentiment = sentiment\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentiment)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = \" \".join(str(self.text[item]).split())\n",
    "        selected_text = \" \".join(str(self.selected_text[item]).split())\n",
    "        len_sel_text = len(selected_text)\n",
    "\n",
    "        idx0 = -1\n",
    "        idx1 = -1\n",
    "        for ind in (i for i, e in enumerate(text) if e == selected_text[0]):\n",
    "            if text[ind:ind+len_sel_text] == selected_text:\n",
    "                idx0 = ind    \n",
    "                idx1 = ind+len_sel_text -1\n",
    "                break\n",
    "\n",
    "        char_target = [0] * len(text)\n",
    "        for j in range(idx0, idx1+1):\n",
    "            if text[j] == ' ':\n",
    "                char_target[j] = 0\n",
    "            else:\n",
    "                char_target[j] = 1\n",
    "\n",
    "        #tok_ids : token_id, ex:101, 1045\n",
    "        #tok_token : token, ex:'[CLS]', 'i',\n",
    "        #tok_offset : (0, 0), (1, 2)\n",
    "        tok_outputs = self.tokenizer.encode(text)\n",
    "        tok_ids = tok_outputs.ids\n",
    "        text_token = tok_outputs.tokens\n",
    "        tok_offset =  tok_outputs.offsets[1:-1]\n",
    "\n",
    "        targets_idx = [] \n",
    "        for j, (offset1, offset2) in enumerate(tok_offset):\n",
    "            if sum(char_target[offset1:offset2]) > 0:\n",
    "                targets_idx.append(j)\n",
    "\n",
    "        \n",
    "        targets_start = np.zeros(self.max_len)\n",
    "        targets_start[targets_idx[0]+1] = 1\n",
    "        targets_end = np.zeros(self.max_len)\n",
    "        targets_end[targets_idx[-1]+1] = 1\n",
    "\n",
    "        #tok_ids = tok_ids \n",
    "        token_type_id = [0] + [1] * (len(tok_ids)-2) + [1]\n",
    "        mask = [1] * len(tok_ids)\n",
    "        tok_offset = [(0,0)] + tok_offset + [(0,0)]\n",
    "\n",
    "        assert len(mask) == len(token_type_id)\n",
    "        assert len(tok_offset) == len(token_type_id) \n",
    "\n",
    "        padding_len = self.max_len - len(tok_ids)\n",
    "        mask = mask + [0] * padding_len \n",
    "        tok_ids = tok_ids + [0] * padding_len\n",
    "        token_type_id = token_type_id + [0] * padding_len \n",
    "        tok_offset = tok_offset + ([(0, 0)] * padding_len)\n",
    "        # targets = targets + [0] * padding_len \n",
    "        # targets_start = targets_start + [0] * padding_len \n",
    "        # targets_end = targets_end + [0] * padding_len \n",
    "\n",
    "        \n",
    "        return {\n",
    "            \"ids\" : torch.tensor(tok_ids, dtype=torch.long),\n",
    "            \"mask_id\" : torch.tensor(mask, dtype=torch.long),\n",
    "            \"token_type_id\" : torch.tensor(token_type_id, dtype=torch.long),\n",
    "            #\"targets\" : torch.tensor(targets, dtype=torch.long),\n",
    "            \"targets_start\" : torch.tensor(targets_start, dtype=torch.long),\n",
    "            \"targets_end\" : torch.tensor(targets_end, dtype=torch.long),\n",
    "            #\"padding_len\" : padding_len,\n",
    "            #\"text_token\" : \" \".join(text_token),\n",
    "            #\"sentiment\" : torch.tensor(sentiment, dtype=torch.float),\n",
    "            \"orig_sentiment\" : self.sentiment[item],\n",
    "            \"orig_text\" : self.text[item],\n",
    "            \"orig_selected_text\" : self.selected_text[item],\n",
    "            \"offset\" : torch.tensor(tok_offset, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "class RoTweetDataset:\n",
    "    def __init__(self, text, selected_text, sentiment, tokenizer, max_len):\n",
    "        self.text = text\n",
    "        self.selected_text = selected_text\n",
    "        self.sentiment = sentiment\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = \" \".join(str(self.text[item]).split())\n",
    "        selected_text = \" \".join(str(self.selected_text[item]).split())\n",
    "        len_sel_text = len(selected_text)\n",
    "\n",
    "        idx0 = None\n",
    "        idx1 = None\n",
    "        for i in (i for i, e in enumerate(text) if e == selected_text[0]):\n",
    "            if text[i:i+len_sel_text] == selected_text:\n",
    "                idx0 = i\n",
    "                idx1 = i+len_sel_text-1\n",
    "                break\n",
    "\n",
    "             \n",
    "        char_targets = [0]*len(text)\n",
    "        for i in range(idx0, idx1+1):\n",
    "            if text[i] != ' ':\n",
    "                char_targets[i] = 1\n",
    "\n",
    "\n",
    "        tok_output = self.tokenizer.encode(text)\n",
    "        orig_ids = tok_output.ids\n",
    "        offsets = tok_output.offsets\n",
    "\n",
    "        targets_idx = []\n",
    "        \n",
    "        for i, (start, end) in enumerate(offsets):\n",
    "            if sum(char_targets[start:end]) > 0:\n",
    "                targets_idx.append(i)\n",
    "\n",
    "\n",
    "        targets_start = np.zeros(self.max_len)\n",
    "        targets_end = np.zeros(self.max_len)\n",
    "        targets_start[targets_idx[0]] = 1\n",
    "        targets_end[targets_idx[-1]] = 1\n",
    "\n",
    "        \n",
    "        ids = [0] + orig_ids + [2] \n",
    "        token_type_ids = [0] + [0] * len(orig_ids) + [0]\n",
    "        mask_ids = len(token_type_ids) * [1]\n",
    "        offsets = [(0,0)] + offsets + [(0,0)]\n",
    "\n",
    "        assert len(ids) == len(token_type_ids)\n",
    "        assert len(token_type_ids) == len(mask_ids)\n",
    "        assert len(mask_ids) == len(offsets)\n",
    "\n",
    "        padding_len = self.max_len - len(ids)\n",
    "\n",
    "        # if padding_len > 0:\n",
    "        #     ids += [0] * padding_len\n",
    "        #     token_type_ids += [0] * padding_len\n",
    "        #     mask_ids += [0] * padding_len\n",
    "        #     offsets += [(0,0)] * padding_len\n",
    "\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids':torch.tensor(ids, dtype=torch.long), \n",
    "            'token_type_ids' : torch.tensor(token_type_ids, dtype=torch.long), \n",
    "            'mask_ids' : torch.tensor(mask_ids, dtype=torch.long), \n",
    "            'offset' : torch.tensor(offsets, dtype=torch.long), \n",
    "            'orig_sentiment' : self.sentiment[item],\n",
    "            'orig_selected_text' : self.selected_text[item],\n",
    "            'orig_text' : self.text[item],\n",
    "            'targets_start' : torch.tensor(targets_start, dtype=torch.long),\n",
    "            'targets_end' : torch.tensor(targets_end, dtype=torch.long),\n",
    "            'text' : tok_output.tokens\n",
    "        }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    trn_df = pd.read_csv(TRAIN_FILE)\n",
    "    test_df = pd.read_csv(\"../input/test.csv\")\n",
    "    test_df['selected_text'] = 'temp'\n",
    "\n",
    "\n",
    "    dataset = RoTweetDataset(trn_df['text'].values, \n",
    "                        trn_df['selected_text'].values, \n",
    "                        trn_df['sentiment'].values, \n",
    "                        ROBERT_TOKENIZER, \n",
    "                        MAX_LEN\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ids', 'token_type_ids', 'mask_ids', 'offset', 'orig_sentiment', 'orig_selected_text', 'orig_text', 'targets_start', 'targets_end', 'text'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'stupid manager charlie'\n",
    "b = 'pathetic person'\n",
    "out = BERT_TOKENIZER.encode(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 5236, 3208, 4918, 102, 17203, 2711, 102]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 1, 1, 1]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'stupid',\n",
       " 'manager',\n",
       " 'charlie',\n",
       " '[SEP]',\n",
       " 'pathetic',\n",
       " 'person',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'stupid manager charlie'\n",
    "out = BERT_TOKENIZER.encode(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# [CLS] SEQ_A [SEP] SEQ_B [SEP]\n",
    "\n",
    "sequence_a = \"HuggingFace is based in NYC\"\n",
    "sequence_b = \"Where is HuggingFace based?\"\n",
    "\n",
    "out1 = tokenizer.encode_plus(sequence_a, sequence_b)\n",
    "#assert tokenizer.decode(encoded_sequence) == \"[CLS] HuggingFace is based in NYC [SEP] Where is HuggingFace based? [SEP]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((128,768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = torch.tensor(np.zeros((128,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = torch.split(out, 1, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.], dtype=torch.float64)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
